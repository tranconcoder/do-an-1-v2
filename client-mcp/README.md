# Aliconcon MCP Client

á»¨ng dá»¥ng client Ä‘á»ƒ káº¿t ná»‘i vá»›i Aliconcon MCP Server sá»­ dá»¥ng Llama 3.2 thÃ´ng qua Ollama.

## ğŸš€ CÃ i Ä‘áº·t

### 1. CÃ i Ä‘áº·t dependencies

```bash
bun install
```

### 2. CÃ i Ä‘áº·t Ollama

```bash
# TrÃªn macOS
brew install ollama

# TrÃªn Linux
curl -fsSL https://ollama.ai/install.sh | sh

# TrÃªn Windows
# Táº£i tá»« https://ollama.ai/download
```

### 3. Táº£i model Llama 3.2

```bash
ollama pull llama3.2:1b
# hoáº·c cho model lá»›n hÆ¡n:
# ollama pull llama3.2:3b
```

### 4. Khá»Ÿi Ä‘á»™ng Ollama server

```bash
ollama serve
```

## ğŸƒâ€â™‚ï¸ Cháº¡y á»©ng dá»¥ng

### BÆ°á»›c 1: Khá»Ÿi Ä‘á»™ng MCP Server

```bash
cd ../server-mcp
bun run server.ts
```

### BÆ°á»›c 2: Khá»Ÿi Ä‘á»™ng Client

```bash
# Terminal má»›i
cd client-mcp
bun start
```

## ğŸ”§ Cáº¥u hÃ¬nh

Báº¡n cÃ³ thá»ƒ tÃ¹y chá»‰nh cáº¥u hÃ¬nh thÃ´ng qua biáº¿n mÃ´i trÆ°á»ng:

```bash
# URL cá»§a MCP server
export MCP_URL="http://localhost:8000/sse"

# URL cá»§a Ollama
export OLLAMA_URL="http://localhost:11434"

# Model Llama sá»­ dá»¥ng
export LLM_MODEL="llama3.2:1b"

# Nhiá»‡t Ä‘á»™ cho model (0.0 - 1.0)
export LLM_TEMPERATURE="0.7"
```

## ğŸ’¬ Sá»­ dá»¥ng

Sau khi khá»Ÿi Ä‘á»™ng, báº¡n cÃ³ thá»ƒ há»i cÃ¡c cÃ¢u há»i nhÆ°:

-   **Giá»›i thiá»‡u vá» Aliconcon**: "Giá»›i thiá»‡u vá» ná»n táº£ng Aliconcon"
-   **Sáº£n pháº©m phá»• biáº¿n**: "Cho tÃ´i xem sáº£n pháº©m bÃ¡n cháº¡y nháº¥t"
-   **ThÃ´ng tin dá»‹ch vá»¥**: "Aliconcon cÃ³ nhá»¯ng tÃ­nh nÄƒng gÃ¬?"
-   **Há»— trá»£ khÃ¡ch hÃ ng**: "LÃ m sao Ä‘á»ƒ mua hÃ ng trÃªn Aliconcon?"

## ğŸ› ï¸ TÃ­nh nÄƒng

-   âœ… Káº¿t ná»‘i vá»›i Aliconcon MCP Server
-   âœ… Sá»­ dá»¥ng Llama 3.2 thÃ´ng qua Ollama
-   âœ… Há»— trá»£ tiáº¿ng Viá»‡t
-   âœ… TÃ­ch há»£p cÃ´ng cá»¥ MCP:
    -   `introduce`: Giá»›i thiá»‡u vá» Aliconcon
    -   `popular-products`: Sáº£n pháº©m phá»• biáº¿n
-   âœ… Giao diá»‡n console thÃ¢n thiá»‡n
-   âœ… Logging chi tiáº¿t
-   âœ… Xá»­ lÃ½ lá»—i robust

## ğŸ” Troubleshooting

### Lá»—i káº¿t ná»‘i Ollama

```bash
# Kiá»ƒm tra Ollama Ä‘ang cháº¡y
curl http://localhost:11434/api/tags

# Khá»Ÿi Ä‘á»™ng láº¡i Ollama
ollama serve
```

### Lá»—i káº¿t ná»‘i MCP Server

```bash
# Kiá»ƒm tra MCP server
curl http://localhost:8000

# Khá»Ÿi Ä‘á»™ng láº¡i MCP server
cd ../server-mcp && bun run server.ts
```

### Model khÃ´ng tá»“n táº¡i

```bash
# Liá»‡t kÃª models cÃ³ sáºµn
ollama list

# Táº£i model cáº§n thiáº¿t
ollama pull llama3.2:1b
```

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
client-mcp/
â”œâ”€â”€ index.ts           # Main client application
â”œâ”€â”€ package.json       # Dependencies vÃ  scripts
â”œâ”€â”€ tsconfig.json      # TypeScript configuration
â”œâ”€â”€ README.md          # Documentation
â””â”€â”€ .gitignore         # Git ignore rules
```

## ğŸ”— LiÃªn quan

-   [Aliconcon MCP Server](../server-mcp/README.md)
-   [Ollama Documentation](https://ollama.ai/docs)
-   [Model Context Protocol](https://modelcontextprotocol.io/)

---

**Aliconcon MCP Client** - Tráº£i nghiá»‡m AI shopping thÃ´ng minh! ğŸ›ï¸ğŸ¤–
